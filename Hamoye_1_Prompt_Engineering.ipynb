{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlr5zdjC408V",
        "outputId": "21df0221-d185-44db-dfaa-18be6e3718a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==v0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==v0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==v0.28.1) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==v0.28.1) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==v0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==v0.28.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==v0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==v0.28.1) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==v0.28.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==v0.28.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==v0.28.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==v0.28.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==v0.28.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==v0.28.1) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.48-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.52-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.5 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.36 langchain-core-0.1.48 langchain-text-splitters-0.0.1 langsmith-0.1.52 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# update or install the necessary libraries\n",
        "!pip install openai==v0.28.1\n",
        "!pip install --upgrade python-dotenv\n",
        "!pip install --upgrade langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "slJF8wqO8FgN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "openai.api_key = \"API_KEY\""
      ],
      "metadata": {
        "id": "YsYieGFm8cmg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_params(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\"set openai parameter\"\"\"\n",
        "    openai_params = {}\n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params\n",
        "\n",
        "def get_completion(params, prompt):\n",
        "    \"\"\"Get completion from openai api\"\"\"\n",
        "    response = openai.Completion.create(\n",
        "        engine = params['model'],\n",
        "        prompt = prompt,\n",
        "        temperature = params['temperature'],\n",
        "        max_tokens = params['max_tokens'],\n",
        "        top_p = params['top_p'],\n",
        "        frequency_penalty = params['frequency_penalty'],\n",
        "        presence_penalty = params['presence_penalty'],\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "8CugXlHC8yAZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_open_params()\n",
        "\n",
        "prompt = \"Roses are\"\n",
        "\n",
        "response = get_completion(params, prompt)"
      ],
      "metadata": {
        "id": "bYm0nNf5_Mni"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MRjwjEX_Cjac",
        "outputId": "36b01c62-0a82-4f5c-a2d0-3b3c08575fc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" red, violets are blue\\nSugar is sweet, and so are you\\nBut the roses wither, the violets die\\nBut my love for you will never say goodbye\\n\\nYour beauty shines like the sun above\\nYour smile is like a ray of love\\nIn your eyes, I see my whole world\\nMy love for you will never be unfurled\\n\\nSo on this day of love and romance\\nI want to take this chance\\nTo say how much you mean to me\\nAnd how my love will always be\\n\\nYou are my forever valentine\\nMy heart, my soul, my everything divine\\nI'll love you always, until the end of time\\nMy dear, my love, my Valentine. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "wbOsB_2KCtmt",
        "outputId": "8abe979d-f86a-4e90-b6e5-5ca313e5ce40"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " red, violets are blue\nSugar is sweet, and so are you\nBut the roses wither, the violets die\nBut my love for you will never say goodbye\n\nYour beauty shines like the sun above\nYour smile is like a ray of love\nIn your eyes, I see my whole world\nMy love for you will never be unfurled\n\nSo on this day of love and romance\nI want to take this chance\nTo say how much you mean to me\nAnd how my love will always be\n\nYou are my forever valentine\nMy heart, my soul, my everything divine\nI'll love you always, until the end of time\nMy dear, my love, my Valentine. "
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_open_params(temperature=0)\n",
        "prompt = \"Roses are\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "VnKeZrYvS5ME",
        "outputId": "a3f5d017-7f7a-4f36-f0ba-25545c626f0a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " red, violets are blue\nSugar is sweet, and so are you\nBut the roses wilt, and the violets fade\nBut my love for you will never degrade\n\nYou are my sunshine on a cloudy day\nMy guiding light when I lose my way\nWith you by my side, I can conquer all\nTogether we stand, we will never fall\n\nYour smile brightens up my darkest night\nYour touch fills me with pure delight\nI am grateful for every moment spent\nWith you, my love, my heart is content\n\nI promise to love you, through thick and thin\nTo be your rock, your shelter, your kin\nI'll hold your hand, and never let go\nTogether we'll face whatever life may throw\n\nSo here's my heart, my love, my all\nWith you, I'll stand tall, I'll never fall\nRoses may wither, and violets may die\nBut my love for you will never say goodbye."
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Summarization"
      ],
      "metadata": {
        "id": "71So2DdaS_Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_open_params()\n",
        "prompt = \"\"\"Keras is a deep learning API written in Python that runs on top of TensorFlow.\n",
        "    It is quite popular among deep learning users because of its ease of use.\n",
        "    TensorFlow is an end-to-end open-source deep learning framework developed and maintained by Google.\n",
        "    Similar to Numpy, TensorFlow allows for mathematical computations and manipulation between numerical tensors, runs on CPUs, GPUs, and TPUs.\n",
        "    Keras was incorporated in TensorFlow 2.0 (the recent version) as tf.keras (high-level API) and can run on the aforementioned hardwares.\n",
        "    TensorFlow also allows for low-level operations with the TensorFlow Core API.\n",
        "\n",
        "    Explain the above in one sentence:\"\"\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "PqtOa4hiS5H8",
        "outputId": "5c4e716b-876d-4352-be25-074a0465e926"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nKeras is a user-friendly deep learning API written in Python that runs on top of TensorFlow, an open-source framework for numerical computations on CPUs, GPUs, and TPUs."
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question Answering"
      ],
      "metadata": {
        "id": "f7qD6hR5Ta6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: The Avengers were a team of extraordinary individuals, with either superpowers or other special characteristics. Though primarily affiliated with the interests of the United States of America, the group's purpose was to protect global stability from inner or extraterrestrial threats. The Avengers were first assembled by S.H.I.E.L.D. as a result of the Avengers Initiative, when Loki invaded Earth with his Chitauri army. The team, consisting of Iron Man, Captain America, Hulk, Thor, Black Widow and Hawkeye defeated Loki and went their separate ways for a while.\n",
        "\n",
        "Question: Why were the Avengers formed?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "mabr0DVWS5D7",
        "outputId": "94997832-86a8-4532-8c8e-517c30533691"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " To protect global stability from inner or extraterrestrial threats."
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Classfication"
      ],
      "metadata": {
        "id": "4AYr7ZuhTw4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the Avengers Endgame was an interesting movie..\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "uJ2DOzNkS48o",
        "outputId": "35b7007d-aae1-4833-da07-32799ddd8331"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Positive"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Role Playing"
      ],
      "metadata": {
        "id": "g_X7lZoNUBfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the big bang theory?\n",
        "AI:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "8XFyGkRDS44b",
        "outputId": "c8486707-b1b0-4fd6-883d-ca119fbfa4cc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " The big bang theory is a scientific model that explains the origin and expansion of the universe. It states that the universe began as a singularity, a point of infinite density and temperature, approximately 13.8 billion years ago. The singularity then expanded rapidly, creating the universe we know today. This theory is supported by evidence such as the cosmic microwave background radiation and the redshift of distant galaxies."
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code Generation"
      ],
      "metadata": {
        "id": "oYdrbGzHUNKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a PostgreSQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "_5l4A69bS41B",
        "outputId": "3025a1c8-eb86-4029-e778-6171d4b0c454"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nSELECT StudentName FROM students\nINNER JOIN departments ON departments.DepartmentId = students.DepartmentId\nWHERE departments.DepartmentName = 'Computer Science'"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reasoning"
      ],
      "metadata": {
        "id": "H-o05T2EUWiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "njrAvNtNS4we",
        "outputId": "b771441c-ea65-48e3-cb56-886cda135ecf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " \n\nStep 1: Identify the odd numbers. \nThe odd numbers in this group are 15, 5, 13, 7, and 1. \n\nStep 2: Add the odd numbers. \n15 + 5 + 13 + 7 + 1 = 41 \n\nStep 3: Determine if the result is odd or even. \nThe sum of the odd numbers is 41, which is an odd number."
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUIZ"
      ],
      "metadata": {
        "id": "WJ6nzpEdB7No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is cloud computing?\"},\n",
        "        {\"role\": \"system\", \"content\": \"Explain the technical concept briefly enough to fit as an MCQ option.\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=30,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_vq4Wa7S4j0",
        "outputId": "b49aab1a-de0c-410c-991d-6385b8ed94c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloud computing is a technology that allows users to access and store data and applications over the internet instead of on a physical hard drive or local server.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"I had an awful experience at the restaurant. The food was bland and the service was slow?\"},\n",
        "        {\"role\": \"system\", \"content\": \"Classify the sentiment of the text provided as positive, negative or neutral.\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=16,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWKH0gqFDBoB",
        "outputId": "a1d58645-fa2d-4b11-9e7a-969a63e60b17"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Who is known as the father of computer science?\"},\n",
        "        {\"role\": \"system\", \"content\": \"Provide a brief, accurate answer to the user’s question that can be used as an MCQ option\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=30,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-s9iZT_E0tB",
        "outputId": "f89c77a5-47f2-4ebc-fa7d-8dab9827dec0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alan Turing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"The meeting was cancelled due to unforeseen circumstances.\"},\n",
        "        {\"role\": \"system\", \"content\": \"Classify the following text's tone in a brief form that could fitas an MCQ option.\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=30,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdVJKFIkE0pL",
        "outputId": "4868f367-c0e8-41de-fa9e-d9286ca1ef12"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What was the main cause of the American Civil War?\"},\n",
        "        {\"role\": \"system\", \"content\": \"Provide a concise fact about the event in history mentioned by the user briefly enough to fit as an MCQ option.\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=40,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvEBf9J6E0lE",
        "outputId": "9c07a834-626c-43f4-9c83-9d73f562f40d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main cause of the American Civil War was slavery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"The economy has been growing steadily over the past few quarters, driven by increases in consumer spending and substantial investments in renewable enrgy infrastructure. Unemployment rates have also fallen to record lows as new industries are creating new jobs.\"},\n",
        "        {\"role\": \"system\", \"content\": \"You are to summarize the provided text into a concise form maintaining the key points and context, briefly enough to fit as an MCQ option.\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=30,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SLr3bihF1PW",
        "outputId": "fa09b347-8822-4e44-87fe-8eb85f43d103"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The economy is growing steadily due to increased consumer spending and investments in renewable energy. Unemployment rates have reached record lows with new industries creating jobs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"How do you say 'I love to travel' in German?.\"},\n",
        "        {\"role\": \"system\", \"content\": \"You will be provided with a sentence in English and your task is to translate it into German\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=64,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HsU51lBG1Kq",
        "outputId": "ae9823ac-0e83-4f4d-c59b-4662b314e0e4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Ich liebe es zu reisen.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What causes the Northern Lights.\"},\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant. A user will ask a question and you should provide a clear and concise answer briefly enough to fit as an MCQ option\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=150,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1WqMGwcJBYt",
        "outputId": "6ac8546e-76e2-4d08-b344-5a64d4544253"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Northern Lights are caused by solar particles colliding with gases in Earth's atmosphere, creating colorful light displays.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model =\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"The internet was developed to help researchers communicate and share computing resources. Today, it connects millions of people and devices across the globe.\"},\n",
        "        {\"role\": \"system\", \"content\": \"Summarize the following into a consise phrase suitable for an MCQ answer option\"}\n",
        "    ],\n",
        "    temperature =0,\n",
        "    max_tokens=30,\n",
        "    top_p=1\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYRq9OD4K3B1",
        "outputId": "3229a8da-6826-47d1-9e32-cf2174e791cc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The internet was developed for research communication and resource sharing, now connecting millions worldwide.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKKo0vEzMI4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}